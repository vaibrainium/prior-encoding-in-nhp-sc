{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec  # Import for custom grid layout\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "import ssm\n",
    "\n",
    "from notebooks.imports import *\n",
    "from config import dir_config, main_config\n",
    "from src.utils import pmf_utils, plot_utils, glm_hmm_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_dir = Path(dir_config.data.compiled)\n",
    "processed_dir = Path(dir_config.data.processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(n_trials, target_fold, k_folds):\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=1)\n",
    "    splits = [(train_idx, test_idx) for train_idx, test_idx in kf.split(np.arange(n_trials))]\n",
    "    return splits[target_fold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def psychometric_fit(stimulus, choices, expected_choice_prob, ax, color, label, n_sample = 10):\n",
    "    \n",
    "    data = {\n",
    "        \"signed_coherence\": np.array(stimulus)*100,\n",
    "        \"choice\": choices\n",
    "    }\n",
    "    x_data, y_data, _, x_model, y_model = pmf_utils.get_psychometric_data(data)\n",
    "    \n",
    "    x_model_hat, y_model_hat = np.full((n_sample,len(x_model)),np.nan), np.full((n_sample,len(y_model)),np.nan)\n",
    "\n",
    "    x_model_hat, y_model_hat = np.full((n_sample,len(x_model)),np.nan), np.full((n_sample,len(y_model)),np.nan)  \n",
    "    for idx_sample in range(n_sample):\n",
    "        data_fitted = {\n",
    "            \"signed_coherence\": np.array(stimulus)*100,\n",
    "            \"choice\": npr.binomial(1, expected_choice_prob)\n",
    "        }\n",
    "        x_data, y_data_hat, _, x_model_hat[idx_sample,:], y_model_hat[idx_sample,:] = pmf_utils.get_psychometric_data(data_fitted)\n",
    "\n",
    "    ax.plot(x_data, y_data,'o', color=color)\n",
    "    ax.plot(x_model, y_model, color=color, label=label)\n",
    "    ax.plot(np.mean(x_model_hat, axis=0), np.mean(y_model_hat, axis=0), color = color, linestyle='--')\n",
    "    ax.fill_between(np.mean(x_model_hat, axis=0), \n",
    "                    np.mean(y_model_hat, axis=0) - np.std(y_model_hat, axis=0), \n",
    "                    np.mean(y_model_hat, axis=0) + np.std(y_model_hat, axis=0),\n",
    "                    color=color, alpha=0.3)\n",
    "\n",
    "    ax.set_xlim(min(x_data),max(x_data))\n",
    "    ax.set_xlabel('Coherence')\n",
    "    ax.set_ylabel('choices toRF')\n",
    "    ax.set_title('Psychometric fits',fontsize=15)\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_model_fits(model, choices, input, stimulus, mask, n_states, session_name, task_switch, trial_indices=None):\n",
    "    transition_matrix = model.transitions.params\n",
    "    transition_matrix = np.exp(transition_matrix)[0]\n",
    "    weights = -model.observations.params\n",
    "    posterior_probs = model.expected_states(data=choices, input=input, mask=np.array(mask).reshape(-1,1))[0]\n",
    "\n",
    "    fig = plt.figure(figsize=(14, 4))  # Wider figure for better spacing\n",
    "    gs = gridspec.GridSpec(1, 4, width_ratios=[1, 1, 1, 2])  # Merge last two subplots\n",
    "\n",
    "    cols = ['#ff7f00', '#4daf4a', '#377eb8']  # Add more colors if needed for higher n_states\n",
    "    plt.suptitle(session_name)\n",
    "    # ----  First Subplot: Psychometric Curves ----\n",
    "    ax1 = plt.subplot(gs[0])\n",
    "\n",
    "    weighted_sum = np.sum(weights * input[None, :, :], axis=-1).T\n",
    "    sigmoid_output = 1 / (1 + np.exp(-weighted_sum)) \n",
    "    glm_hmm_expected_choice_prob = np.sum(sigmoid_output * posterior_probs, axis=1, keepdims=True)\n",
    "    \n",
    "    equal_indices = np.where(mask)[0][:task_switch]\n",
    "    unequal_indices = np.where(mask)[0][task_switch:]\n",
    "    if trial_indices is not None:\n",
    "        equal_indices = np.intersect1d(trial_indices, equal_indices)\n",
    "        unequal_indices = np.intersect1d(trial_indices, unequal_indices)\n",
    "    \n",
    "    psychometric_fit(stimulus[equal_indices], choices[equal_indices], glm_hmm_expected_choice_prob[equal_indices], ax1, '#377eb8', label=\"Equal\")\n",
    "    psychometric_fit(stimulus[unequal_indices], choices[unequal_indices], glm_hmm_expected_choice_prob[unequal_indices], ax1, '#974810', label=\"Unequal\")\n",
    "    \n",
    "    # ---- Second Subplot: GLM Weights ----\n",
    "    ax2 = plt.subplot(gs[1])\n",
    "    for k in range(n_states):\n",
    "        ax2.plot(np.arange(input.shape[1]), weights[k][0], marker='o',\n",
    "                color=cols[k], linestyle='-', lw=1.5, label=f\"State {k+1}\")\n",
    "\n",
    "    ax2.tick_params(axis='y', labelsize=10)\n",
    "    ax2.set_ylabel(\"GLM weight\", fontsize=15)\n",
    "    ax2.set_xlabel(\"covariate\", fontsize=15)\n",
    "    ax2.set_xticks(range(input.shape[1]))\n",
    "    ax2.set_xticklabels(['stimulus', 'bias', 'previous choice', 'previous target'], fontsize=12, rotation=15)\n",
    "    ax2.axhline(y=0, color=\"k\", alpha=0.5, ls=\"--\")\n",
    "    ax2.legend()\n",
    "    ax2.set_title(\"GLM weights\", fontsize=15)\n",
    "\n",
    "    # ---- Third Subplot: Transition Matrix ----\n",
    "    ax3 = plt.subplot(gs[2])\n",
    "    im = ax3.imshow(transition_matrix, vmin=-0.8, vmax=1, cmap='bone')\n",
    "    for i in range(transition_matrix.shape[0]):\n",
    "        for j in range(transition_matrix.shape[1]):\n",
    "            ax3.text(j, i, str(np.around(transition_matrix[i, j], decimals=2)), \n",
    "                    ha=\"center\", va=\"center\", color=\"k\", fontsize=12)\n",
    "\n",
    "    ax3.set_xlim(-0.5, n_states - 0.5)\n",
    "    ax3.set_ylim(n_states - 0.5, -0.5)\n",
    "    ax3.set_xticks(range(n_states))\n",
    "    ax3.set_yticks(range(n_states))\n",
    "    ax3.set_xlabel(\"state t+1\", fontsize=15)\n",
    "    ax3.set_ylabel(\"state t\", fontsize=15)\n",
    "    ax3.set_title(\"Generative transition matrix\", fontsize=15)\n",
    "\n",
    "    # ---- Fourth (Merged) Subplot: Posterior Probabilities ----\n",
    "    ax4 = plt.subplot(gs[3])  # Merged across two columns\n",
    "    for k in range(n_states):\n",
    "        ax4.plot(posterior_probs[mask, k], label=f\"State {k + 1}\", lw=2, color=cols[k])\n",
    "\n",
    "    ax4.set_ylim(-0.01, 1.01)\n",
    "    ax4.set_yticks([0, 0.5, 1])\n",
    "    ax4.tick_params(axis='y', labelsize=10)\n",
    "    ax4.set_xlabel(\"trial #\", fontsize=15)\n",
    "    ax4.set_ylabel(\"p(state)\", fontsize=15)\n",
    "    ax4.axvline(x=task_switch, color=\"k\", alpha=0.5, ls=\"--\")\n",
    "    ax4.legend()\n",
    "    ax4.set_title(\"Posterior Probabilities\", fontsize=15)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(processed_dir, 'glm_hmm_all_trials.pkl'), 'rb') as f:\n",
    "    glm_hmm = pickle.load(f)\n",
    "\n",
    "session_metadata = pd.read_csv(Path(compiled_dir,'sessions_metadata.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_state in range(glm_hmm[\"session_wise\"]['test_ll'].shape[1]):\n",
    "    print(f\"Test likelihood for {n_state+2} states: {np.mean(glm_hmm['session_wise']['test_ll'][:,n_state,:])}\")\n",
    "best_state_idx = np.argmax(np.mean(glm_hmm[\"session_wise\"]['test_ll'],axis=(0,2))) \n",
    "best_state = best_state_idx + 2\n",
    "print(f\"Best state is {best_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best fold for each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fold_session_wise = []\n",
    "for session in range(glm_hmm[\"session_wise\"]['test_ll'].shape[0]):\n",
    "    best_fold_session_wise.append(np.argmax(glm_hmm[\"session_wise\"][\"test_ll\"][session,best_state_idx,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get train/test  split for each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold, test_fold = {}, {}\n",
    "for idx_session, session in enumerate(session_metadata['session_id']):\n",
    "    n_trials = len(glm_hmm[\"session_wise\"][\"data\"][session][\"choices\"])\n",
    "    best_fold = best_fold_session_wise[idx_session]\n",
    "    train_fold[session], test_fold[session] = get_train_test_split(n_trials=n_trials, target_fold=best_fold, k_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fits and data recovery (train and test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Psychometric Fits, GLM weights, transition matrix, p(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_session, session in enumerate(session_metadata['session_id']):\n",
    "    \n",
    "    train_idx = train_fold[session]\n",
    "    \n",
    "    model = glm_hmm[\"session_wise\"]['models'][idx_session][best_state][best_fold_session_wise[idx_session]]\n",
    "    choices = glm_hmm[\"session_wise\"][\"data\"][session][\"choices\"].values.reshape(-1, 1)\n",
    "    input = np.array(glm_hmm[\"session_wise\"][\"data\"][session][[\"normalized_stimulus\",\"bias\",\"previous_choice\",\"previous_target\"]])\n",
    "\n",
    "\n",
    "    stimulus = glm_hmm[\"session_wise\"][\"data\"][session][\"stimulus\"]\n",
    "\n",
    "    \n",
    "    if glm_hmm[\"session_wise\"][\"data\"][session][\"mask\"] is None:\n",
    "        mask = None\n",
    "    else:\n",
    "        mask = glm_hmm[\"session_wise\"][\"data\"][session][\"mask\"]\n",
    "    mask = np.ones_like(choices, dtype=bool) if mask is None else mask\n",
    "\n",
    "    prob_toRF = glm_hmm[\"session_wise\"][\"data\"][session][\"prob_toRF\"]\n",
    "    prob_toRF = prob_toRF[mask]\n",
    "    task_switch = np.where((prob_toRF != 50) & ~np.isnan(prob_toRF))[0][0]\n",
    "    plot_model_fits(model, choices, input, stimulus, mask, best_state, session, task_switch, trial_indices=train_idx)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx_session, session in enumerate(session_metadata['session_id']):\n",
    "    \n",
    "    test_idx = test_fold[session]\n",
    "    \n",
    "    model = glm_hmm[\"session_wise\"]['models'][idx_session][best_state][best_fold_session_wise[idx_session]]\n",
    "    choices = glm_hmm[\"session_wise\"][\"data\"][session][\"choices\"].values.reshape(-1, 1)\n",
    "    input = np.array(glm_hmm[\"session_wise\"][\"data\"][session][[\"normalized_stimulus\",\"bias\",\"previous_choice\",\"previous_target\"]])\n",
    "\n",
    "\n",
    "    stimulus = glm_hmm[\"session_wise\"][\"data\"][session][\"stimulus\"]\n",
    "\n",
    "    \n",
    "    mask = glm_hmm[\"session_wise\"][\"data\"][session][\"mask\"]\n",
    "    mask = np.ones_like(choices, dtype=bool) if mask is None else mask\n",
    "\n",
    "    prob_toRF = glm_hmm[\"session_wise\"][\"data\"][session][\"prob_toRF\"]\n",
    "    prob_toRF = prob_toRF[mask]\n",
    "    task_switch = np.where((prob_toRF != 50) & ~np.isnan(prob_toRF))[0][0]\n",
    "    plot_model_fits(model, choices, input, stimulus, mask, best_state, session, task_switch, trial_indices=test_idx)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit on whole session and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params['transition_matrices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get parameter initializations for each session\n",
    "init_params = {\n",
    "    'glm_weights': {},\n",
    "    'transition_matrices': {}\n",
    "}\n",
    "choices_session_wise, inputs_session_wise, masks_session_wise = [], [], []\n",
    "\n",
    "for idx_session, session in enumerate(session_metadata['session_id']):\n",
    "    init_params['glm_weights'][idx_session] = glm_hmm[\"session_wise\"]['models'][idx_session][best_state][best_fold_session_wise[idx_session]].observations.params\n",
    "    init_params['transition_matrices'][idx_session] = glm_hmm[\"session_wise\"]['models'][idx_session][best_state][best_fold_session_wise[idx_session]].transitions.params\n",
    "    \n",
    "    \n",
    "    choices_session_wise.append(glm_hmm[\"session_wise\"][\"data\"][session][\"choices\"].values.reshape(-1, 1))\n",
    "    inputs_session_wise.append(np.array(glm_hmm[\"session_wise\"][\"data\"][session][[\"normalized_stimulus\",\"bias\",\"previous_choice\",\"previous_target\"]]))\n",
    "    masks_session_wise.append(glm_hmm[\"session_wise\"][\"data\"][session][\"mask\"].values.reshape(-1, 1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_session, fit_ll_session = glm_hmm_utils.session_wise_fit(choices_session_wise, inputs_session_wise, masks=masks_session_wise,\n",
    "                                                                                    n_sessions=len((session_metadata['session_id'])), init_params=init_params, n_states = best_state, n_iters= 1000)\n",
    "\n",
    "models_session_wise, fit_lls_session_wise ={},{}\n",
    "for idx_session, session_id in enumerate(session_metadata['session_id']):\n",
    "    models_session_wise[session_id] = models_session[idx_session]\n",
    "    fit_lls_session_wise[session_id] = fit_ll_session[idx_session]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_session, session in enumerate(session_metadata['session_id']):\n",
    "        \n",
    "    model = models_session_wise[session]\n",
    "    choices = glm_hmm[\"session_wise\"][\"data\"][session][\"choices\"].values.reshape(-1, 1)\n",
    "    input = np.array(glm_hmm[\"session_wise\"][\"data\"][session][[\"normalized_stimulus\",\"bias\",\"previous_choice\",\"previous_target\"]])\n",
    "\n",
    "\n",
    "    stimulus = glm_hmm[\"session_wise\"][\"data\"][session][\"stimulus\"]\n",
    "\n",
    "    \n",
    "    mask = glm_hmm[\"session_wise\"][\"data\"][session][\"mask\"]\n",
    "    mask = np.ones_like(choices, dtype=bool) if mask is None else mask\n",
    "    \n",
    "    prob_toRF = glm_hmm[\"session_wise\"][\"data\"][session][\"prob_toRF\"]\n",
    "    prob_toRF = prob_toRF[mask]\n",
    "    task_switch = np.where((prob_toRF != 50) & ~np.isnan(prob_toRF))[0][0]\n",
    "    plot_model_fits(model, choices, input, stimulus, mask, best_state, session, task_switch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store data and models for session-wise\n",
    "session_wise_fits = {\n",
    "    'models': models_session_wise,\n",
    "    'fit_lls': fit_lls_session_wise,\n",
    "    'data': glm_hmm[\"session_wise\"]['data'],\n",
    "}\n",
    "\n",
    "\n",
    "best_idx = glm_hmm[\"global\"][\"fits_lls_glm_hmm\"][best_state].index(max(glm_hmm[\"global\"][\"fits_lls_glm_hmm\"][best_state]))\n",
    "global_fits = {\n",
    "    'model': glm_hmm[\"global\"][\"models\"][best_state][best_idx],\n",
    "    'fit_ll': max(glm_hmm[\"global\"][\"fits_lls_glm_hmm\"][best_state][best_idx]),\n",
    "    'data': glm_hmm[\"global\"][\"data\"]\n",
    "}\n",
    "\n",
    "models_and_data = {\n",
    "    'global': global_fits,\n",
    "    'session_wise': session_wise_fits,\n",
    "}\n",
    "\n",
    "with open(Path(processed_dir, f'glm_hmm_all_trials_final.pkl'), 'wb') as f:\n",
    "    pickle.dump(models_and_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
