{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from notebooks.imports import *\n",
    "from config import dir_config, main_config\n",
    "from src.utils import pmf_utils, plot_utils\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_dir = Path(dir_config.data.compiled)\n",
    "processed_dir = Path(dir_config.data.processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(processed_dir, 'glm_hmm_all_trials.pkl'), 'rb') as f:\n",
    "    glm_hmm = pickle.load(f)\n",
    "\n",
    "session_metadata = pd.read_csv(Path(compiled_dir,'sessions_metadata.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_state in range(glm_hmm[\"session_wise\"]['test_ll'].shape[1]):\n",
    "    print(f\"Test likelihood for {n_state+2} states: {np.mean(glm_hmm['session_wise']['test_ll'][:,n_state,:])}\")\n",
    "best_state = np.argmax(np.mean(glm_hmm[\"session_wise\"]['test_ll'],axis=(0,2))) \n",
    "print(f\"Best state is {best_state+2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best fold for each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fold_session_wise = []\n",
    "for session in range(glm_hmm[\"session_wise\"]['test_ll'].shape[0]):\n",
    "    best_fold_session_wise.append(np.argmax(glm_hmm[\"session_wise\"][\"test_ll\"][session,best_state,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model verification \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def psychometric_fit(model, choices, prob_choice_hat, stimulus, ax, color, label, n_sample = 10):\n",
    "    \n",
    "    data = {\n",
    "        \"signed_coherence\": np.array(stimulus)*100,\n",
    "        \"choice\": choices\n",
    "    }\n",
    "    x_data, y_data, _, x_model, y_model = pmf_utils.get_psychometric_data(data)\n",
    "    \n",
    "    x_model_hat, y_model_hat = np.full((n_sample,len(x_model)),np.nan), np.full((n_sample,len(y_model)),np.nan)\n",
    "\n",
    "    x_model_hat, y_model_hat = np.full((n_sample,len(x_model)),np.nan), np.full((n_sample,len(y_model)),np.nan)  \n",
    "    for idx_sample in range(n_sample):\n",
    "        data_fitted = {\n",
    "            \"signed_coherence\": np.array(stimulus)*100,\n",
    "            \"choice\": npr.binomial(1, prob_choice_hat)\n",
    "        }\n",
    "        x_data, y_data_hat, _, x_model_hat[idx_sample,:], y_model_hat[idx_sample,:] = pmf_utils.get_psychometric_data(data_fitted)\n",
    "\n",
    "    ax.plot(x_data, y_data,'o', color=color)\n",
    "    ax.plot(x_model, y_model, color=color, label=label)\n",
    "    ax.plot(np.mean(x_model_hat, axis=0), np.mean(y_model_hat, axis=0), color = color, linestyle='--')\n",
    "    ax.fill_between(np.mean(x_model_hat, axis=0), \n",
    "                    np.mean(y_model_hat, axis=0) - np.std(y_model_hat, axis=0), \n",
    "                    np.mean(y_model_hat, axis=0) + np.std(y_model_hat, axis=0),\n",
    "                    color=color, alpha=0.3)\n",
    "\n",
    "    ax.set_xlim(min(x_data),max(x_data))\n",
    "    ax.set_xlabel('Coherence')\n",
    "    ax.set_ylabel('choices toRF')\n",
    "    ax.set_title('Psychometric fits',fontsize=15)\n",
    "    ax.legend()\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec  # Import for custom grid layout\n",
    "\n",
    "def plot_model_fits(model, choices, input, stimulus, mask, n_states, session_name, task_switch):\n",
    "    transition_matrix = model.transitions.params\n",
    "    transition_matrix = np.exp(transition_matrix)[0]\n",
    "    weights = -model.observations.params\n",
    "    posterior_probs = model.expected_states(data=choices, input=input, mask=np.array(mask).reshape(-1,1))[0]\n",
    "\n",
    "    fig = plt.figure(figsize=(14, 4))  # Wider figure for better spacing\n",
    "    gs = gridspec.GridSpec(1, 4, width_ratios=[1, 1, 1, 2])  # Merge last two subplots\n",
    "\n",
    "    cols = ['#ff7f00', '#4daf4a', '#377eb8']  # Add more colors if needed for higher n_states\n",
    "    plt.suptitle(session_name)\n",
    "    # ----  First Subplot: Psychometric Curves ----\n",
    "    ax1 = plt.subplot(gs[0])\n",
    "\n",
    "    weighted_sum = np.sum(weights * input[None, :, :], axis=-1).T\n",
    "    sigmoid_output = 1 / (1 + np.exp(-weighted_sum)) \n",
    "    prob_choice_hat = np.sum(sigmoid_output * posterior_probs, axis=1, keepdims=True)\n",
    "    \n",
    "    psychometric_fit(model, choices[mask][:task_switch,:], prob_choice_hat[mask][:task_switch,:], stimulus[mask][:task_switch], ax1, '#377eb8', label=\"Equal\")\n",
    "    psychometric_fit(model, choices[mask][task_switch:,:], prob_choice_hat[mask][task_switch:,:], stimulus[mask][task_switch:], ax1, '#974810', label=\"Unequal\")\n",
    "    \n",
    "    # ---- Second Subplot: GLM Weights ----\n",
    "    ax2 = plt.subplot(gs[1])\n",
    "    for k in range(n_states):\n",
    "        ax2.plot(np.arange(input.shape[1]), weights[k][0], marker='o',\n",
    "                color=cols[k], linestyle='-', lw=1.5, label=f\"State {k+1}\")\n",
    "\n",
    "    ax2.tick_params(axis='y', labelsize=10)\n",
    "    ax2.set_ylabel(\"GLM weight\", fontsize=15)\n",
    "    ax2.set_xlabel(\"covariate\", fontsize=15)\n",
    "    ax2.set_xticks(range(input.shape[1]))\n",
    "    ax2.set_xticklabels(['stimulus', 'bias', 'previous choice', 'previous target'], fontsize=12, rotation=15)\n",
    "    ax2.axhline(y=0, color=\"k\", alpha=0.5, ls=\"--\")\n",
    "    ax2.legend()\n",
    "    ax2.set_title(\"GLM weights\", fontsize=15)\n",
    "\n",
    "    # ---- Third Subplot: Transition Matrix ----\n",
    "    ax3 = plt.subplot(gs[2])\n",
    "    im = ax3.imshow(transition_matrix, vmin=-0.8, vmax=1, cmap='bone')\n",
    "    for i in range(transition_matrix.shape[0]):\n",
    "        for j in range(transition_matrix.shape[1]):\n",
    "            ax3.text(j, i, str(np.around(transition_matrix[i, j], decimals=2)), \n",
    "                    ha=\"center\", va=\"center\", color=\"k\", fontsize=12)\n",
    "\n",
    "    ax3.set_xlim(-0.5, n_states - 0.5)\n",
    "    ax3.set_ylim(n_states - 0.5, -0.5)\n",
    "    ax3.set_xticks(range(n_states))\n",
    "    ax3.set_yticks(range(n_states))\n",
    "    ax3.set_xlabel(\"state t+1\", fontsize=15)\n",
    "    ax3.set_ylabel(\"state t\", fontsize=15)\n",
    "    ax3.set_title(\"Generative transition matrix\", fontsize=15)\n",
    "\n",
    "    # ---- Fourth (Merged) Subplot: Posterior Probabilities ----\n",
    "    ax4 = plt.subplot(gs[3])  # Merged across two columns\n",
    "    for k in range(n_states):\n",
    "        ax4.plot(posterior_probs[mask, k], label=f\"State {k + 1}\", lw=2, color=cols[k])\n",
    "\n",
    "    ax4.set_ylim(-0.01, 1.01)\n",
    "    ax4.set_yticks([0, 0.5, 1])\n",
    "    ax4.tick_params(axis='y', labelsize=10)\n",
    "    ax4.set_xlabel(\"trial #\", fontsize=15)\n",
    "    ax4.set_ylabel(\"p(state)\", fontsize=15)\n",
    "    ax4.axvline(x=task_switch, color=\"k\", alpha=0.5, ls=\"--\")\n",
    "    ax4.legend()\n",
    "    ax4.set_title(\"Posterior Probabilities\", fontsize=15)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GLM weights, transition matrix, p(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### session-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_state=1\n",
    "for idx_session, session in enumerate(session_metadata['session_id']):\n",
    "    \n",
    "    model = glm_hmm[\"session_wise\"]['models'][idx_session][best_state+2][best_fold_session_wise[idx_session]]\n",
    "    choices = glm_hmm[\"session_wise\"][\"data\"][session][\"choices\"].values.reshape(-1, 1)\n",
    "    input = np.array(glm_hmm[\"session_wise\"][\"data\"][session][[\"normalized_stimulus\",\"bias\",\"previous_choice\",\"previous_target\"]])\n",
    "\n",
    "\n",
    "    stimulus = glm_hmm[\"session_wise\"][\"data\"][session][\"stimulus\"]\n",
    "\n",
    "    \n",
    "    if glm_hmm[\"session_wise\"][\"data\"][session][\"mask\"] is None:\n",
    "        mask = None\n",
    "    else:\n",
    "        mask = glm_hmm[\"session_wise\"][\"data\"][session][\"mask\"]\n",
    "    mask = np.ones_like(choices, dtype=bool) if mask is None else mask\n",
    "\n",
    "    prob_toRF = glm_hmm[\"session_wise\"][\"data\"][session][\"prob_toRF\"]\n",
    "    prob_toRF = prob_toRF[mask]\n",
    "    task_switch = np.where((prob_toRF != 50) & ~np.isnan(prob_toRF))[0][0]\n",
    "    plot_model_fits(model, choices, input, stimulus, mask, best_state+2, session, task_switch)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data recovery (train and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
