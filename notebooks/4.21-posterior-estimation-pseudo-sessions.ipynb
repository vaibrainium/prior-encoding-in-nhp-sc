{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.init import normal, constant\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "# from attorch.train import early_stopping\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook', font_scale=1.3)\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import numpy.matlib\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from notebooks.imports import *\n",
    "from config import dir_config\n",
    "from src.utils.glm_hmm_utils import *\n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.bayesian_encoding.model import Net\n",
    "from src.bayesian_encoding.utils import set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_dir = Path(dir_config.data.compiled)\n",
    "processed_dir = Path(dir_config.data.processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reject sessions\n",
    "session_to_exclude = [\"210210_GP_JP\",\"241209_GP_TZ\"]\n",
    "\n",
    "session_metadata = pd.read_csv(Path(compiled_dir, \"sessions_metadata.csv\"))\n",
    "neuron_metadata = pd.read_csv(Path(compiled_dir, \"neuron_metadata.csv\"))\n",
    "\n",
    "session_metadata = session_metadata[~session_metadata[\"session_id\"].isin(session_to_exclude)]\n",
    "neuron_metadata = neuron_metadata[~neuron_metadata[\"session_id\"].isin(session_to_exclude)]\n",
    "\n",
    "with open(Path(processed_dir, \"glm_hmm_all_trials_prior_based_initialization_final.pkl\"), \"rb\") as f:\n",
    "\tglm_hmm = pickle.load(f)\n",
    "\n",
    "with open(Path(processed_dir, \"ephys_neuron_wise.pkl\"), \"rb\") as f:\n",
    "\tephys = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coh = [-50, -20, -6, 0, 6, 20, 50]\n",
    "\n",
    "equal_prior = np.array([0.125, .125, .125, .25, .125, .125, .125])\n",
    "toRF_prior = np.array([0.075, 0.075, 0.075, 0.25, 0.175, 0.175, 0.175])\n",
    "awayRF_prior = np.array([0.175, 0.175, 0.175, 0.25, 0.075, 0.075, 0.075])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_neural_data(epoch, valid_trial_filter, neuron_ids):\n",
    "\t\"\"\"return 3d array of shape (num_neurons, num_trials, num_time_bins)\"\"\"\n",
    "\tneural_data = []\n",
    "\tfor neuron_id in neuron_ids:\n",
    "\t\tneural_data.append(ephys[epoch][neuron_id]['convolved_spike_trains'])\n",
    "\tneural_data = np.array(neural_data)\n",
    "\tneural_data = neural_data[:, valid_trial_filter, :]\n",
    "\treturn neural_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toRF_trial_freq_by_coh_and_prior = np.empty((len(session_metadata), 2, len(coh)))*np.nan\n",
    "awayRF_trial_freq_by_coh_and_prior = np.empty((len(session_metadata), 2, len(coh)))*np.nan\n",
    "# aggregate all sessions\n",
    "for session_idx, session_id in enumerate(session_metadata.session_id):\n",
    "\ttrial_data = glm_hmm['session_wise']['data'][session_id]\n",
    "\tvalid_trial_filter = np.where((trial_data['mask'] == 1) & (~np.isnan(trial_data['reaction_time'])))[0]\n",
    "\ttrial_data = trial_data.iloc[valid_trial_filter]\n",
    "\ttrial_data_count = trial_data.groupby(['prob_toRF', 'stimulus']).size().reset_index(name='freq')\n",
    "\n",
    "\tif session_metadata[session_metadata[\"session_id\"] == session_id].prior_direction.values[0] == \"toRF\":\n",
    "\t\ttoRF_trial_freq_by_coh_and_prior[session_idx, :, :] = np.array(trial_data_count.freq).reshape(2, len(coh))\n",
    "\telif session_metadata[session_metadata[\"session_id\"] == session_id].prior_direction.values[0] == \"awayRF\":\n",
    "\t\tawayRF_trial_freq_by_coh_and_prior[session_idx, :, :] = np.array(trial_data_count.freq).reshape(2, len(coh))[::-1]\n",
    "\n",
    "toRF_min_trial_per_condition = np.nanmin(toRF_trial_freq_by_coh_and_prior, axis=0).astype(int)\n",
    "awayRF_min_trial_per_condition = np.nanmin(awayRF_trial_freq_by_coh_and_prior, axis=0).astype(int)\n",
    "toRF_min_trial_per_condition, awayRF_min_trial_per_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 'response_onset'\n",
    "agg_toRF_neuron_data = []\n",
    "agg_awayRF_neuron_data = []\n",
    "\n",
    "# randomly draw trials from each session\n",
    "for session_idx, session_id in enumerate(session_metadata.session_id):\n",
    "\ttrial_data = glm_hmm['session_wise']['data'][session_id]\n",
    "\tvalid_trial_filter = np.where((trial_data['mask'] == 1) & (~np.isnan(trial_data['reaction_time'])))[0]\n",
    "\ttrial_data = trial_data.iloc[valid_trial_filter]\n",
    "\tneuron_ids = neuron_metadata.neuron_id[neuron_metadata.session_id == session_id].values\n",
    "\n",
    "\ttoRF_trial_indices = {'eq': [], 'uneq': []}\n",
    "\tawayRF_trial_indices = {'eq': [], 'uneq': []}\n",
    "\n",
    "\t# randomly draw n trials from each session\n",
    "\tfor coh_idx, coh_val in enumerate(coh):\n",
    "\t\tequal_trials = np.where((trial_data['prob_toRF'] == 50) & (trial_data['stimulus'] == coh_val/100))[0]\n",
    "\t\tunequal_trials = np.where((trial_data['prob_toRF'] != 50) & (trial_data['stimulus'] == coh_val/100))[0]\n",
    "\n",
    "\t\tif session_metadata['prior_direction'].iloc[session_idx] == \"toRF\":\n",
    "\t\t\ttoRF_trial_indices['eq'].append(npr.choice(equal_trials, size=toRF_min_trial_per_condition[0, coh_idx], replace=False))\n",
    "\t\t\ttoRF_trial_indices['uneq'].append(npr.choice(unequal_trials, size=toRF_min_trial_per_condition[1, coh_idx], replace=False))\n",
    "\n",
    "\t\telif session_metadata['prior_direction'].iloc[session_idx] == \"awayRF\":\n",
    "\t\t\tawayRF_trial_indices['eq'].append(npr.choice(equal_trials, size=awayRF_min_trial_per_condition[0, coh_idx], replace=False))\n",
    "\t\t\tawayRF_trial_indices['uneq'].append(npr.choice(unequal_trials, size=awayRF_min_trial_per_condition[1, coh_idx], replace=False))\n",
    "\n",
    "\n",
    "\tif session_metadata['prior_direction'].iloc[session_idx] == \"toRF\":\n",
    "\t\tall_trial_indices = np.hstack(toRF_trial_indices['eq'] + toRF_trial_indices['uneq'])\n",
    "\t\tagg_toRF_neuron_data.append(get_session_neural_data(epoch=epoch, valid_trial_filter=all_trial_indices, neuron_ids=neuron_ids))\n",
    "\telif session_metadata['prior_direction'].iloc[session_idx] == \"awayRF\":\n",
    "\t\tall_trial_indices = np.hstack(awayRF_trial_indices['eq'] + awayRF_trial_indices['uneq'])\n",
    "\t\tagg_awayRF_neuron_data.append(get_session_neural_data(epoch=epoch, valid_trial_filter=all_trial_indices, neuron_ids=neuron_ids))\n",
    "\n",
    "agg_toRF_neuron_data = np.vstack(agg_toRF_neuron_data)\n",
    "agg_awayRF_neuron_data = np.vstack(agg_awayRF_neuron_data)\n",
    "agg_toRF_neuron_data.shape, agg_awayRF_neuron_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_toRF = np.vstack((np.vstack([np.matlib.repmat(equal_prior,trial_num,1) for trial_num in toRF_min_trial_per_condition[0,:]]),\n",
    "                       np.vstack([np.matlib.repmat(toRF_prior,trial_num,1) for trial_num in toRF_min_trial_per_condition[1,:]])))\n",
    "\n",
    "prior_awayRF = np.vstack((np.vstack([np.matlib.repmat(equal_prior,trial_num,1) for trial_num in awayRF_min_trial_per_condition[0,:]]),\n",
    "                       np.vstack([np.matlib.repmat(awayRF_prior,trial_num,1) for trial_num in awayRF_min_trial_per_condition[1,:]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_toRF = np.repeat([coh,coh],toRF_min_trial_per_condition.flatten())\n",
    "\n",
    "t_awayRF = np.repeat([coh,coh],awayRF_min_trial_per_condition.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### toRF session fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_map = {stim: i for i, stim in enumerate(np.sort(coh))}\n",
    "\n",
    "x = torch.tensor(np.nansum(agg_toRF_neuron_data[:, :, 200:300], axis=2).T, dtype=torch.float32, device=\"cuda\")\n",
    "t = torch.tensor([stim_map[stim] for stim in t_toRF], dtype=torch.long, device=\"cuda\")\n",
    "prior = torch.tensor(prior_toRF, dtype=torch.float32, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(n_channel=x.shape[1], n_output=len(coh), n_hidden=[500,500], std=0.001, dropout=0.5)\n",
    "net.cuda()\n",
    "loss = nn.CrossEntropyLoss().cuda()\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 1\n",
    "net.eval()\n",
    "y = net(x)\n",
    "y = y + prior\n",
    "val, idx = torch.max(y, 1)\n",
    "yd = y.data.cpu().numpy()\n",
    "\n",
    "\n",
    "plt.subplot(211)\n",
    "t_hat = idx.data.cpu().numpy()\n",
    "plt.scatter(t_hat, t.cpu().numpy())\n",
    "\n",
    "print(np.sqrt(np.mean((t_hat - t.cpu().numpy())**2)) * delta)\n",
    "\n",
    "plt.subplot(212)\n",
    "\n",
    "plt.plot(yd[47])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.std = 1e-2\n",
    "set_seed(8)\n",
    "net.initialize()\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Learning rates\n",
    "alpha = 1  # Reduce regularization term\n",
    "learning_rates = 0.003 * 3.0**(-np.arange(3))  # Reduced learning rates\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print('\\n\\n\\n\\n LEARNING RATE: {}'.format(lr))\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        net.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y = net(x)\n",
    "\n",
    "        # Check for NaNs in output\n",
    "        if torch.isnan(y).any() or torch.isinf(y).any():\n",
    "            print(f\"NaNs or Infs detected in output y at epoch {epoch}\")\n",
    "            print(f\"Output y range: min={y.min()}, max={y.max()}, mean={y.mean()}\")\n",
    "            break\n",
    "\n",
    "        post = y\n",
    "        post = post - post.max(1, keepdim=True)[0]  # Stabilize softmax\n",
    "\n",
    "        # Check for NaNs in post\n",
    "        if torch.isnan(post).any() or torch.isinf(post).any():\n",
    "            print(f\"NaNs or Infs detected in post at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        conv_filter = torch.tensor([-0.25, 0.5, -0.25], dtype=torch.float32, device=\"cuda\").reshape(1, 1, -1)\n",
    "        smoothness = nn.functional.conv1d(y.unsqueeze(1), conv_filter).pow(2).mean()\n",
    "\n",
    "        score = loss(post, t) + alpha * smoothness\n",
    "\n",
    "        # Check for NaNs in loss score\n",
    "        if torch.isnan(score).any() or torch.isinf(score).any():\n",
    "            print(f\"NaNs or Infs detected in loss at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        score.backward()\n",
    "\n",
    "        # Check gradients for NaNs\n",
    "        for param in net.parameters():\n",
    "            if torch.isnan(param.grad).any():\n",
    "                print(\"NaNs detected in gradients\")\n",
    "                break\n",
    "\n",
    "        # # Apply gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)  # You can experiment with max_norm\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Score: {score.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(net, x=None, t=None):\n",
    "\tif x is None and t is None:\n",
    "\t\treturn None\n",
    "\tnet.eval()\n",
    "\ty = net(x)\n",
    "\tposterior = y + prior\n",
    "\t_, loc = torch.max(posterior, dim=1)\n",
    "\tv =(t.double() - loc.double()).pow(2).mean().sqrt() * delta\n",
    "\treturn v.data.cpu().numpy()\n",
    "\n",
    "objective(net, x=x, t=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 7\n",
    "pv = (np.arange(nbins) - nbins//2) * delta\n",
    "t_ = t.cuda().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import count\n",
    "\n",
    "y = net(x)\n",
    "#val, idx = torch.max(y, 1)\n",
    "yd = y.data.cpu().numpy()\n",
    "yd = np.exp(yd)\n",
    "yd = yd / yd.sum(axis=1, keepdims=True)\n",
    "\n",
    "loc = yd.argmax(axis=1)\n",
    "ds = (np.arange(nbins) - loc[:, None])**2\n",
    "\n",
    "avg_sigma = np.mean(np.sqrt(np.sum(yd * ds, axis=1))) * delta\n",
    "\n",
    "n_rows = 5\n",
    "n_cols = 6\n",
    "fig, axs = plt.subplots(5, 6, figsize=(12, 12))\n",
    "\n",
    "for c, ((i, j), ax) in zip(count(), np.ndenumerate(axs)):\n",
    "    c = c+400\n",
    "\n",
    "    ax.plot(pv, yd[c])\n",
    "    ax.scatter(pv[t_[c]],0.4, color='red', marker='x', s=1)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_yticks([])\n",
    "    if j == 0 and i == n_rows-1:\n",
    "        ax.set_ylabel('Likelihood (a.u.)')\n",
    "        ax.set_xlabel('Coherence (%)')#($^\\circ$)')\n",
    "    else:\n",
    "        ax.set_xticklabels([])\n",
    "\n",
    "fig.suptitle(\"$\\mathbf{{E}}[\\sigma]$={:0.3f}\".format(avg_sigma), fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
