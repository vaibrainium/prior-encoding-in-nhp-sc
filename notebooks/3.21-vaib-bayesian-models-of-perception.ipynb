{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for <br> Vincent 2015: A tutorial on Bayesian Models of Perception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from notebooks.imports import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import arviz as az\n",
    "import pytensor\n",
    "import scipy\n",
    "\n",
    "# setting float precision in pytensor\n",
    "pytensor.config.floatX = \"float32\"\n",
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", False)\n",
    "# check if GPU is available\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Currently using: {torch.cuda.get_device_name(0)} device\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import dir_config, main_config\n",
    "\n",
    "raw_dir = Path(dir_config.data.raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "\n",
    "\n",
    "def m1_posterior_prediction(T, si, variance):\n",
    "    \"\"\"\n",
    "    Generate random samples of k based on the given parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - T: Number of trials\n",
    "    - si: Stimulus intensities (scalar or array)\n",
    "    - variance: Variance parameter (scalar)\n",
    "\n",
    "    Returns:\n",
    "    - k: Random samples of k based on the generated binomial distribution\n",
    "    - PCc: Probability of correct response (scalar or array)\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate PC using normal CDF\n",
    "    PCc = scipy.stats.norm.cdf(si / np.sqrt(2 * variance))\n",
    "    \n",
    "    # Sample from binomial distribution\n",
    "    k = np.random.binomial(T, PCc)\n",
    "\n",
    "    return k, PCc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### True Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_variance = 1\n",
    "true_T = 100\n",
    "true_si = np.logspace(-2, 2, 10)\n",
    "true_bias = 0\n",
    "true_lapse_rate = 0.01\n",
    "true_prior = np.array([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Statistical Decision Theory with Bayesian estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Simulate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # For reproducibility\n",
    "data = {\n",
    "    \"sioriginal\": true_si, # example stimulus intensity\n",
    "    \"T\": true_T, # number of trials\n",
    "    \"muN\": 0, # mean of the prior\n",
    "    \"v\": true_variance, # standard deviation of the prior\n",
    "}\n",
    "data[\"ni\"] = 41 # number of iterations\n",
    "data[\"sii\"] =  np.logspace(-2, 2, data[\"ni\"]) #  interpolated stimulus intensities\n",
    "data[\"si\"] = np.concatenate([data['sioriginal'], data['sii']]) \n",
    "# Generate data['koriginal'] by simulating counts\n",
    "data[\"koriginal\"], _ = m1_posterior_prediction(data['T'], data['sioriginal'], data['v'])\n",
    "\n",
    "plt.plot(data['sioriginal'], data['koriginal'], 'o')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Parameter Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model and calculate the joint probability and posterior\n",
    "with pm.Model() as model1:\n",
    "    # Prior\n",
    "    variance = pm.Uniform(\"variance\", lower=0, upper=1000)\n",
    "    \n",
    "    # PCc using Normal CDF\n",
    "    sioriginal = pm.Data(\"sioriginal\", data[\"sioriginal\"])\n",
    "    PCc = pm.Deterministic(\"PCc\", pm.math.invlogit(sioriginal / pm.math.sqrt(2 * variance)))\n",
    "    T = pm.Data(\"T\", data[\"T\"])\n",
    "    \n",
    "    # likelihood\n",
    "    kc = pm.Binomial(\"kc\", n=T, p=PCc, observed=data[\"koriginal\"])\n",
    "    \n",
    "model1.to_graphviz()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model1:\n",
    "    trace = pm.sample(draws=1000, tune=500, chains=2, cores=4, target_accept=0.95, compute_convergence_checks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract samples from the trace\n",
    "variance_samples = trace.posterior[\"variance\"].values.flatten()\n",
    "# Plotting posterior distributions\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.hist(variance_samples, bins=30, density=True, alpha=0.7)\n",
    "plt.title('Posterior Distribution of Variance')\n",
    "plt.xlabel('Variance')\n",
    "plt.ylabel('Density')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the variance samples from the trace\n",
    "variance_samples = trace.posterior[\"variance\"].values.flatten()\n",
    "\n",
    "# Generate simulated data using posterior samples\n",
    "def generate_simulated_data(T, si, variance_samples):\n",
    "    simulated_k = np.zeros((len(variance_samples), len(si)), dtype=int)\n",
    "    \n",
    "    for i, variance in enumerate(variance_samples):\n",
    "        k, _ = m1_posterior_prediction(T, si, variance)\n",
    "        simulated_k[i] = k\n",
    "        \n",
    "    return simulated_k.mean(axis=0)\n",
    "\n",
    "simulated_k = generate_simulated_data(data['T'], data['si'], variance_samples)\n",
    "\n",
    "# Plotting simulated data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(data['si'], simulated_k, marker='o', linestyle='', color='b', label='Simulated Data')\n",
    "plt.plot(data['sioriginal'], data['koriginal'], marker='o', linestyle=\"\", color='r', label='Original Data')\n",
    "plt.title('Simulated Data')\n",
    "plt.xlabel('Stimulus Intensity')\n",
    "plt.ylabel('Number of Correct Responses')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: A Trial-to-trial SDT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Simulate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"lr\"] = true_lapse_rate\n",
    "data[\"bias\"] = true_bias\n",
    "data[\"prior\"] = true_prior\n",
    "data[\"si\"] = np.concatenate([data[\"sioriginal\"], data[\"sioriginal\"], data[\"sii\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"sioriginal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytensor.tensor as tt\n",
    "import pymc as pm\n",
    "\n",
    "# Assuming 'data' is defined elsewhere in your script\n",
    "# and it contains 'prior', 'si', and 'koriginal'\n",
    "\n",
    "with pm.Model() as model2:\n",
    "    # Prior\n",
    "    bias = pm.Normal(\"bias\", mu=0, sigma=1/2000)\n",
    "    lapse_rate = pm.Beta(\"lapse_rate\", alpha=1, beta=1)\n",
    "    variance = pm.Uniform(\"variance\", lower=0, upper=1000)\n",
    "    target_location = pm.Categorical(\"prior_probability\", p=data[\"prior\"])\n",
    "    \n",
    "    # Likelihoods\n",
    "    mu_x_perceived = pm.math.switch(tt.eq(target_location, 1), data[\"sioriginal\"], 0)\n",
    "    x_perceived = pm.Normal(\"x_perceived\", mu=mu_x_perceived, sigma=1/variance)\n",
    "    \n",
    "    # Compute mct with compatible shapes\n",
    "    condition = x_perceived - bias > 0  # condition should be a boolean array\n",
    "    p_true = tt.stack([1 - lapse_rate/2, lapse_rate/2])  # shape (2,)\n",
    "    p_false = tt.stack([lapse_rate/2, 1 - lapse_rate/2])  # shape (2,)\n",
    "    \n",
    "    # Ensure condition is reshaped correctly for broadcasting\n",
    "    condition_broadcasted = tt.shape_padleft(condition)\n",
    "    \n",
    "    # Ensure p_true and p_false are reshaped to match condition_broadcasted\n",
    "    p_true_broadcasted = tt.shape_padright(p_true, 1)\n",
    "    p_false_broadcasted = tt.shape_padright(p_false, 1)\n",
    "    \n",
    "    # Use the switch function with the reshaped condition and p_true, p_false\n",
    "    mct = pm.Deterministic('mct', pm.math.switch(condition_broadcasted, p_true_broadcasted, p_false_broadcasted))\n",
    "    \n",
    "    # Response posterior\n",
    "    response = pm.Categorical(\"response\", p=mct, observed=data[\"koriginal\"])\n",
    "\n",
    "model2.to_graphviz()\n",
    "with model2:\n",
    "    trace = pm.sample(draws=1000, tune=500, chains=2, cores=4, target_accept=0.95, compute_convergence_checks=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
