{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ssm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import stats\n",
    "\n",
    "from notebooks.imports import *\n",
    "from config import dir_config, main_config\n",
    "from src.utils.glm_hmm_utils import *\n",
    "import pickle\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_dir = Path(dir_config.data.compiled)\n",
    "processed_dir = Path(dir_config.data.processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_TRIALS = 'valid_only'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_previous_data(trial_data, invalid_idx):\n",
    "    npr.seed(1)\n",
    "    prev_choice = np.hstack([trial_data.choice[0], trial_data.choice[:-1]])  # 0:awayRF, 1:toRF of previous valid trial\n",
    "    prev_target = np.hstack([trial_data.target[0], trial_data.target[:-1]]) * 2 - 1  # -1:awayRF, 1:toRF of previous valid trial\n",
    "\n",
    "    # indices where the previous trial is invalid/valid\n",
    "    prev_invalid_idx = np.array(invalid_idx) + 1\n",
    "    if 0 in invalid_idx:\n",
    "        prev_invalid_idx = np.append(0, prev_invalid_idx)\n",
    "    prev_valid_idx = np.setdiff1d(np.arange(len(trial_data)), prev_invalid_idx)\n",
    "\n",
    "    for i in prev_invalid_idx[prev_invalid_idx < len(trial_data)]:\n",
    "        if i < prev_valid_idx[0]: #randomly sample if no previous valid trials\n",
    "            prev_choice[i] = np.random.binomial(1,0.5)\n",
    "            prev_target[i] = np.random.binomial(1,0.5) * 2 - 1\n",
    "        else:\n",
    "            last_valid =  np.where(prev_valid_idx<i)[0][-1]\n",
    "            prev_choice[i] = prev_choice[prev_valid_idx[last_valid]]\n",
    "            prev_target[i] = prev_target[prev_valid_idx[last_valid]]\n",
    "\n",
    "    prev_choice = (prev_choice * 2) - 1 # -1:awayRF, 1:toRF of previous valid trial\n",
    "    return prev_choice.astype(int), prev_target.astype(int)\n",
    "\n",
    "\n",
    "\n",
    "def prepare_input_data(data, input_dim, invalid_idx):\n",
    "    X = np.ones((1, data.shape[0], input_dim))\n",
    "    \n",
    "    current_stimulus = data.coherence * (2*data.target-1)\n",
    "    current_stimulus = current_stimulus / 100\n",
    "    \n",
    "    X[0,:,0] = current_stimulus\n",
    "    X[0,:,2], X[0,:,3] = extract_previous_data(data, invalid_idx)\n",
    "    return list(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create design matrix (input, output, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 2       # number of discrete states\n",
    "obs_dim = 1           # number of observed dimensions: choice(toRF/awayRF)\n",
    "num_categories = 2    # number of categories for output\n",
    "input_dim = 4        # input dimensions: current signed coherence, 1(bias), previous choice(toRF/awayRF), previous target side(toRF/awayRF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_metadata = pd.read_csv(Path(compiled_dir, \"sessions_metadata.csv\"), index_col=None)\n",
    "inputs_session_wise = []\n",
    "choices_session_wise = []\n",
    "invalid_idx_session_wise = []\n",
    "masks_session_wise = []\n",
    "GP_trial_num_session_wise = []\n",
    "prob_toRF_session_wise = []\n",
    "\n",
    "for session_id in (session_metadata['session_id']):\n",
    "\n",
    "    trial_data = pd.read_csv(Path(compiled_dir, session_id,f\"{session_id}_trial.csv\"), index_col=None)\n",
    "    GP_trial_data = trial_data[trial_data.task_type == 1].reset_index()\n",
    "    GP_trial_data.choice = GP_trial_data.choice.fillna(-1)\n",
    "    GP_trial_data.target = GP_trial_data.target.fillna(-1)\n",
    "    GP_trial_data.outcome = GP_trial_data.outcome.fillna(-1)\n",
    "\n",
    "    invalid_idx = np.where(GP_trial_data.outcome < 0)[0]\n",
    "    valid_idx = np.where(GP_trial_data.outcome >= 0)[0]\n",
    "\n",
    "    inputs = prepare_input_data(GP_trial_data, input_dim, invalid_idx)\n",
    "    choices = GP_trial_data.choice.values.reshape(-1,1).astype('int')\n",
    "    \n",
    "    if _TRIALS == 'all_trials':\n",
    "        # for training, replace -1 with random sample from 0,1\n",
    "        choices[choices == -1] = npr.choice(1,invalid_idx.shape[0])\n",
    "        mask = np.ones_like(choices, dtype=bool)\n",
    "        mask[invalid_idx] = 0\n",
    "        GP_trial_num = np.array(GP_trial_data.trial_number)\n",
    "        prob_toRF = np.array(GP_trial_data.prob_toRF)\n",
    "        \n",
    "    elif _TRIALS == 'valid_only':\n",
    "        choices = choices[valid_idx,:]\n",
    "        inputs[0] = inputs[0][valid_idx,:]\n",
    "        mask = np.ones_like(choices, dtype=bool)\n",
    "        GP_trial_num = np.array(GP_trial_data.trial_number)[valid_idx]\n",
    "        prob_toRF = np.array(GP_trial_data.prob_toRF)[valid_idx]\n",
    "        \n",
    "    masks_session_wise.append(mask)\n",
    "    inputs_session_wise += inputs\n",
    "    choices_session_wise.append(choices)\n",
    "    GP_trial_num_session_wise.append(GP_trial_num)\n",
    "    prob_toRF_session_wise.append(prob_toRF)\n",
    "\n",
    "\n",
    "inputs_aggregated, choices_aggregated, masks_aggregated = [], [], []\n",
    "inputs_aggregated.append(np.vstack(inputs_session_wise))\n",
    "choices_aggregated.append(np.vstack(choices_session_wise))\n",
    "masks_aggregated.append(np.vstack(masks_session_wise))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalized_inputs_aggregated = copy.deepcopy(inputs_aggregated)\n",
    "unnormalized_inputs_session_wise = copy.deepcopy(inputs_session_wise)\n",
    "# scaling signed coherence \n",
    "inputs_aggregated[0][masks_aggregated[0][:,0],0] = preprocessing.scale(inputs_aggregated[0][masks_aggregated[0][:,0], 0], axis=0)\n",
    "for idx_session in range(len(session_metadata)):\n",
    "    inputs_session_wise[idx_session][masks_session_wise[idx_session][:,0],0] = preprocessing.scale(\n",
    "        inputs_session_wise[idx_session][masks_session_wise[idx_session][:,0], 0], axis=0) # normalize signed coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_glm_hmm, fit_lls_glm_hmm = global_fit(choices_aggregated, inputs_aggregated, masks=masks_aggregated, n_iters= 1000, n_initializations=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best model of 20 initializations for each state\n",
    "init_params = {\n",
    "    'glm_weights': {},\n",
    "    'transition_matrices': {}\n",
    "}\n",
    "for n_states in np.arange(2,6):\n",
    "    best_idx = fit_lls_glm_hmm[n_states].index(max(fit_lls_glm_hmm[n_states]))\n",
    "    init_params['glm_weights'][n_states] = models_glm_hmm[n_states][best_idx].observations.params\n",
    "    init_params['transition_matrices'][n_states] = models_glm_hmm[n_states][best_idx].transitions.params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session-wise fitting with 5 fold cross-validation\n",
    "models_session_state_fold, train_ll_session, test_ll_session = session_wise_fit_cv(choices_session_wise, inputs_session_wise, masks=masks_session_wise,\n",
    "                                                                                    n_sessions=len((session_metadata['session_id'])), init_params=init_params, n_iters= 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store data and models for aggregated\n",
    "agg_data = pd.DataFrame({\n",
    "    \"choices\": choices_aggregated[0].reshape(-1),\n",
    "    \"stimulus\": unnormalized_inputs_aggregated[0][:,0],\n",
    "    \"normalized_stimulus\": inputs_aggregated[0][:,0],\n",
    "    \"bias\": inputs_aggregated[0][:,1],\n",
    "    \"previous_choice\": inputs_aggregated[0][:,2],\n",
    "    \"previous_target\": inputs_aggregated[0][:,3],\n",
    "    'mask': masks_aggregated[0].reshape(-1),\n",
    "})\n",
    "global_fits = {\n",
    "    'models': models_glm_hmm,\n",
    "    'fits_lls_glm_hmm': fit_lls_glm_hmm,\n",
    "    \"data\": agg_data\n",
    "}\n",
    "\n",
    "\n",
    "# store data and models for session-wise\n",
    "session_data = {}\n",
    "for idx_session, session_id in enumerate(session_metadata['session_id']):\n",
    "    session_data[session_id] = pd.DataFrame({\n",
    "        \"choices\": choices_session_wise[idx_session].reshape(-1),\n",
    "        \"stimulus\": unnormalized_inputs_session_wise[idx_session][:,0],\n",
    "        \"normalized_stimulus\": inputs_session_wise[idx_session][:,0],\n",
    "        \"bias\": inputs_session_wise[idx_session][:,1],\n",
    "        \"previous_choice\": inputs_session_wise[idx_session][:,2],\n",
    "        \"previous_target\": inputs_session_wise[idx_session][:,3],\n",
    "        \"mask\": masks_session_wise[idx_session].reshape(-1),\n",
    "        \"trial_num\":GP_trial_num_session_wise[idx_session].reshape(-1),\n",
    "        \"prob_toRF\": prob_toRF_session_wise[idx_session].reshape(-1),\n",
    "    })\n",
    "\n",
    "session_wise_fits = {\n",
    "    'models': models_session_state_fold,\n",
    "    'train_ll': train_ll_session,\n",
    "    'test_ll': test_ll_session,\n",
    "    'data': session_data,\n",
    "}\n",
    "\n",
    "\n",
    "models_and_data = {\n",
    "    'global': global_fits,\n",
    "    'session_wise': session_wise_fits,\n",
    "}\n",
    "\n",
    "with open(Path(processed_dir, f'glm_hmm_{_TRIALS}.pkl'), 'wb') as f:\n",
    "    pickle.dump(models_and_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
